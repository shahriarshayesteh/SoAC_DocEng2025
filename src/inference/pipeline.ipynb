{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268f937c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import shutil\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import wandb\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Inlined MySingleDataset\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class MySingleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, mode, b_path, layer_to_select=-1):\n",
    "        self.sents_reps = torch.load(b_path + f\"{mode}_sents.pt\")\n",
    "        self.labels = torch.load(b_path + f\"{mode}_labels.pt\")\n",
    "        self.sample_num = self.labels.shape[0]\n",
    "        self.layer_to_select = layer_to_select\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.sents_reps.dim() == 3:\n",
    "            layer_embedding = self.sents_reps[index, self.layer_to_select, :]\n",
    "        else:\n",
    "            layer_embedding = self.sents_reps[index, :]\n",
    "        return layer_embedding, self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.sample_num\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Inlined DownstreamModelSingle\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "class DownstreamModelSingle(nn.Module):\n",
    "    def __init__(self, embed_size: int, class_num: int, common_dim: int = None):\n",
    "        super().__init__()\n",
    "        if common_dim and (common_dim != embed_size):\n",
    "            self.compress = nn.Sequential(\n",
    "                nn.Linear(embed_size, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "            )\n",
    "            final_input_dim = common_dim\n",
    "        else:\n",
    "            self.compress = nn.Identity()\n",
    "            final_input_dim = embed_size\n",
    "\n",
    "        self.fc1 = nn.Linear(final_input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.compress(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return self.fc3(out)\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Inlined Train & Test from model_ops_single\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def Train(dataloader, device, model, loss_fn, optimizer, batch_num, wandb_logger=None):\n",
    "    model.train()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss, num_batches = 0.0, 0\n",
    "\n",
    "    for batch_emb, batch_labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        batch_emb = batch_emb.to(device).float()\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        logits = model(batch_emb)\n",
    "        loss = loss_fn(logits, batch_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        pred_y = torch.argmax(logits, dim=1)\n",
    "        all_preds.extend(pred_y.cpu().numpy())\n",
    "        all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    metrics = {\n",
    "        \"train_loss\": avg_loss,\n",
    "        \"train_accuracy\": acc,\n",
    "        \"train_macro_f1\": f1_score(all_labels, all_preds, average=\"macro\"),\n",
    "        \"train_micro_f1\": f1_score(all_labels, all_preds, average=\"micro\"),\n",
    "        \"train_weighted_f1\": f1_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        \"train_weighted_precision\": precision_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        \"train_weighted_recall\": recall_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        \"train_weighted_accuracy\": acc,\n",
    "    }\n",
    "    if wandb_logger:\n",
    "        wandb_logger.log(metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def Test(dataloader, device, model, loss_fn, batch_num, wandb_logger=None, mode=\"val\"):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss, num_batches = 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_emb, batch_labels in tqdm(dataloader, desc=f\"Evaluating({mode})\"):\n",
    "            batch_emb = batch_emb.to(device).float()\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            logits = model(batch_emb)\n",
    "            loss = loss_fn(logits, batch_labels)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            pred_y = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(pred_y.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    metrics = {\n",
    "        f\"{mode}_loss\": avg_loss,\n",
    "        f\"{mode}_accuracy\": acc,\n",
    "        f\"{mode}_macro_f1\": f1_score(all_labels, all_preds, average=\"macro\"),\n",
    "        f\"{mode}_micro_f1\": f1_score(all_labels, all_preds, average=\"micro\"),\n",
    "        f\"{mode}_weighted_f1\": f1_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        f\"{mode}_weighted_precision\": precision_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        f\"{mode}_weighted_recall\": recall_score(all_labels, all_preds, average=\"weighted\"),\n",
    "        f\"{mode}_weighted_accuracy\": acc,\n",
    "    }\n",
    "    if wandb_logger:\n",
    "        wandb_logger.log(metrics)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 1. LABEL MAPPING (same as before)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "LABEL_MAPPING = {\n",
    "    \"finance, marketing & human resources\": 0,\n",
    "    \"information technology & electronics\": 1,\n",
    "    \"consumer & supply chain\": 2,\n",
    "    \"civil, mechanical & electrical\": 3,\n",
    "    \"medical\": 4,\n",
    "    \"sports, media & entertainment\": 5,\n",
    "    \"education\": 6,\n",
    "    \"government, defense & legal\": 7,\n",
    "    \"travel, food & hospitality\": 8,\n",
    "    \"non-profit\": 9,\n",
    "}\n",
    "REVERSE_LABEL_MAPPING = {v: k for k, v in LABEL_MAPPING.items()}\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Evaluation & Save Predictions (same as before)\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def evaluate_and_save_single(dataloader, model, device, results_dir):\n",
    "    model.eval()\n",
    "    all_probs, all_preds, all_labels = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy().tolist())\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"True Label\": [REVERSE_LABEL_MAPPING[l] for l in all_labels],\n",
    "        \"Predicted Label\": [REVERSE_LABEL_MAPPING[p] for p in all_preds],\n",
    "        \"Probabilities\": all_probs,\n",
    "    })\n",
    "    df.to_csv(os.path.join(results_dir, \"predictions.csv\"), index=False)\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    w_acc = balanced_accuracy_score(all_labels, all_preds)\n",
    "    w_prec = precision_score(all_labels, all_preds, average=\"weighted\")\n",
    "    w_rec = recall_score(all_labels, all_preds, average=\"weighted\")\n",
    "    w_f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "\n",
    "    with open(os.path.join(results_dir, \"test_results.txt\"), \"w\") as f:\n",
    "        f.write(f\"Overall Accuracy:  {acc:.4f}\\n\")\n",
    "        f.write(f\"Weighted Accuracy: {w_acc:.4f}\\n\")\n",
    "        f.write(f\"Weighted Precision: {w_prec:.4f}\\n\")\n",
    "        f.write(f\"Weighted Recall:   {w_rec:.4f}\\n\")\n",
    "        f.write(f\"Weighted F1:       {w_f1:.4f}\\n\")\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, digits=4)\n",
    "    with open(os.path.join(results_dir, \"class_based_results.txt\"), \"w\") as f:\n",
    "        f.write(report)\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    class_names = [None] * len(LABEL_MAPPING)\n",
    "    for name, idx in LABEL_MAPPING.items():\n",
    "        class_names[idx] = name\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=class_names,\n",
    "        yticklabels=class_names,\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.savefig(os.path.join(results_dir, \"confusion_matrix.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"[✔] Evaluation artifacts saved to {results_dir}\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 3. TRAINING LOGIC + PUSH TO HUB\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def run_single(args):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dataset_root = os.path.join(args.embeddings_root, args.dataset_subdir, \"dataset_tensor/\")\n",
    "    results_dir = os.path.join(args.results_root, args.model_variant + \"_single\")\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=args.wandb_project,\n",
    "        entity=args.wandb_entity,\n",
    "        name=args.wandb_name,\n",
    "        dir=args.log_dir,\n",
    "        config={\n",
    "            \"embed_size\": args.embed_size,\n",
    "            \"common_dim\": args.common_dim,\n",
    "            \"epochs\": args.epochs,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"learning_rate\": args.learning_rate,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Load datasets & dataloaders\n",
    "    train_ds = MySingleDataset(\"train\", b_path=dataset_root)\n",
    "    val_ds = MySingleDataset(\"validation\", b_path=dataset_root)\n",
    "    test_ds = MySingleDataset(\"test\", b_path=dataset_root)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, args.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    # Build model\n",
    "    model = DownstreamModelSingle(args.embed_size, class_num=10, common_dim=args.common_dim).to(device)\n",
    "    wandb.watch(model, log=\"all\")\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    best_loss = float(\"inf\")\n",
    "    best_model_fp = os.path.join(results_dir, \"best_model.pt\")\n",
    "\n",
    "    # Training loop\n",
    "    for ep in range(1, args.epochs + 1):\n",
    "        print(f\"\\n=== Epoch {ep}/{args.epochs} ===\")\n",
    "        train_metrics = Train(train_loader, device, model, loss_fn, optimizer, 10, wandb)\n",
    "        print(\"Train:\", train_metrics)\n",
    "\n",
    "        if train_metrics[\"train_loss\"] < best_loss:\n",
    "            best_loss = train_metrics[\"train_loss\"]\n",
    "            torch.save(model.state_dict(), best_model_fp)\n",
    "\n",
    "        if ep % args.val_check_interval == 0:\n",
    "            val_metrics = Test(val_loader, device, model, loss_fn, 10, wandb, mode=\"val\")\n",
    "            print(\"Val:\", val_metrics)\n",
    "\n",
    "    # Final Test + Save predictions\n",
    "    if os.path.exists(best_model_fp):\n",
    "        model.load_state_dict(torch.load(best_model_fp, map_location=device))\n",
    "    Test(test_loader, device, model, loss_fn, 10, wandb, mode=\"test\")\n",
    "    evaluate_and_save_single(test_loader, model, device, results_dir)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    # ──────────────────────────────────────────────────────────────────────────────\n",
    "    # 4. PUSH THE BEST MODEL TO HUGGING FACE HUB (raw PyTorch approach)\n",
    "    # ──────────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    from huggingface_hub import Repository\n",
    "\n",
    "    HF_USERNAME = \"Shahriar\"  # e.g. \"shahriarshayesteh\"\n",
    "    REPO_NAME = \"SoACer\"\n",
    "    REPO_URL = f\"https://huggingface.co/{HF_USERNAME}/{REPO_NAME}\"\n",
    "\n",
    "    # 4.1) Create (or re‐use) a local folder for the HF repo\n",
    "    local_repo_dir = os.path.join(results_dir, \"hf_repo_soacer\")\n",
    "    repo = Repository(\n",
    "        local_dir=local_repo_dir,\n",
    "        clone_from=REPO_URL,\n",
    "        use_auth_token=True\n",
    "    )\n",
    "\n",
    "    # 4.2) Copy the best_model.pt into that repo as \"pytorch_model.bin\"\n",
    "    dst_path = os.path.join(local_repo_dir, \"pytorch_model.bin\")\n",
    "    shutil.copy(best_model_fp, dst_path)\n",
    "\n",
    "    # 4.3) Write a minimal config.json if you want (optional).  \n",
    "    #       This can store hyperparameters such as embed_size, common_dim, class_num.\n",
    "    config = {\n",
    "        \"embed_size\": args.embed_size,\n",
    "        \"common_dim\": args.common_dim,\n",
    "        \"class_num\": 10,\n",
    "        \"architecture\": \"DownstreamModelSingle\"\n",
    "    }\n",
    "    import json\n",
    "    with open(os.path.join(local_repo_dir, \"config.json\"), \"w\") as fp:\n",
    "        json.dump(config, fp, indent=2)\n",
    "\n",
    "    # 4.4) Write a README.md explaining usage\n",
    "    readme_text = f\"\"\"\\\n",
    "    # SoACer\n",
    "\n",
    "    This Hugging Face repository contains the best‐performing \n",
    "    `DownstreamModelSingle` classifier head trained on SoAC embeddings.\n",
    "\n",
    "    ## Model Files\n",
    "     - `pytorch_model.bin`: PyTorch `state_dict()` of the classifier head.\n",
    "     - `config.json`: JSON with model hyperparameters (embed_size, common_dim, etc.).\n",
    "     - `README.md`: Usage instructions.\n",
    "\n",
    "    ## How to load\n",
    "\n",
    "    ```python\n",
    "    import torch\n",
    "    from DownstreamModelSingle import DownstreamModelSingle\n",
    "\n",
    "    # 1) Clone the repo\n",
    "    #    git clone https://huggingface.co/{HF_USERNAME}/{REPO_NAME}\n",
    "\n",
    "    # 2) Load config.json\n",
    "    import json\n",
    "    cfg = json.load(open(\"{REPO_NAME}/config.json\"))\n",
    "\n",
    "    model = DownstreamModelSingle(\n",
    "        embed_size=cfg[\"embed_size\"],\n",
    "        class_num=cfg[\"class_num\"],\n",
    "        common_dim=cfg[\"common_dim\"]\n",
    "    )\n",
    "\n",
    "    # 3) Load weights\n",
    "    state_dict = torch.load(\"{REPO_NAME}/pytorch_model.bin\", map_location=\"cpu\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Use `model` on new precomputed embeddings:\n",
    "    #    given `new_embedding`: a 1D Tensor of size (embed_size,)\n",
    "    #    logits = model(new_embedding.unsqueeze(0))  # shape (1, class_num)\n",
    "    #    probs = torch.softmax(logits, dim=-1)\n",
    "    # ```\n",
    "    \"\"\"\n",
    "    with open(os.path.join(local_repo_dir, \"README.md\"), \"w\") as fp:\n",
    "        fp.write(readme_text)\n",
    "\n",
    "    # 4.5) Commit & push to the Hub\n",
    "    repo.push_to_hub(commit_message=\"Upload best SoACer classifier head\")\n",
    "    print(f\"[✔] Pushed classifier head to https://huggingface.co/{HF_USERNAME}/{REPO_NAME}\")\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# 5. ARGPARSE\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Train single-embedding classifier.\")\n",
    "\n",
    "    parser.add_argument(\"--model_variant\", type=str, required=True)\n",
    "    parser.add_argument(\"--embed_size\", type=int, required=True)\n",
    "    parser.add_argument(\"--common_dim\", type=int, default=512)\n",
    "    parser.add_argument(\"--dataset_subdir\", type=str, required=True)\n",
    "\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=1e-4)\n",
    "    parser.add_argument(\"--val_check_interval\", type=int, default=1)\n",
    "\n",
    "    parser.add_argument(\"--embeddings_root\", type=str, required=True)\n",
    "    parser.add_argument(\"--results_root\", type=str, required=True)\n",
    "    parser.add_argument(\"--wandb_project\", type=str, default=\"SoAC\")\n",
    "    parser.add_argument(\"--wandb_name\", type=str, required=True)\n",
    "    parser.add_argument(\"--log_dir\", type=str, default=\"./wandb_logs\")\n",
    "    parser.add_argument(\"--wandb_entity\", type=str, default=None)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     args = parse_args()\n",
    "#     run_single(args)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n",
    "\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "args = SimpleNamespace(\n",
    "    model_variant=\"Meta-Llama-3-8B\",\n",
    "    embed_size=4096,\n",
    "    common_dim=4096,\n",
    "    dataset_subdir=\"Meta-Llama-3-8B/\",  # e.g., \"ablation\", \"summary\", etc.\n",
    "    epochs=10,\n",
    "    batch_size=8,\n",
    "    learning_rate=2e-4,\n",
    "    val_check_interval=1,\n",
    "    embeddings_root=\"/data/sxs7285/Porjects_code/thesis/DocEng/classification/embeddings/model_embeddings/\",  # adjust as needed\n",
    "    results_root=\"/data/sxs7285/Projects_code/thesis/SoAC/resultss\",\n",
    "    wandb_project=\"SoAC\",\n",
    "    wandb_name=\"ablation_single_2048_512\",\n",
    "    log_dir=\"./wandb_logs\",\n",
    "    wandb_entity=\"shahriar92\",  # Optional, if you're part of an org\n",
    ")\n",
    "\n",
    "run_single(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4901426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✔] Repo created: https://huggingface.co/Shahriar/SoACer\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import HfApi, HfFolder\n",
    "\n",
    "# # Get your token (assumes you're logged in already)\n",
    "# token = HfFolder.get_token()\n",
    "\n",
    "# # Set your repo and user/org\n",
    "# username = \"Shahriar\"              # your HF username (not display name)\n",
    "# repo_name = \"SoACer\"\n",
    "# repo_id = f\"{username}/{repo_name}\"\n",
    "\n",
    "# # Create the repo using updated API\n",
    "# api = HfApi()\n",
    "# repo_url = api.create_repo(\n",
    "#     repo_id=repo_id,\n",
    "#     token=token,\n",
    "#     exist_ok=False,     # raises error if repo exists\n",
    "#     private=False       # set to True if you want the repo to be private\n",
    "# )\n",
    "\n",
    "# print(f\"[✔] Repo created: {repo_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a9f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from collections import defaultdict\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoModel, \n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "class SoACerCrawler:\n",
    "    def __init__(self, user_agent=\"SoACerBot\"):\n",
    "        self.user_agent = user_agent\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({'User-Agent': self.user_agent})\n",
    "\n",
    "    def is_allowed_by_robots(self, url):\n",
    "        parsed_url = urlparse(url)\n",
    "        robots_url = f\"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt\"\n",
    "        rp = RobotFileParser()\n",
    "        try:\n",
    "            rp.set_url(robots_url)\n",
    "            rp.read()\n",
    "            return rp.can_fetch(self.user_agent, url)\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to parse robots.txt for {url}: {e}\")\n",
    "            return False\n",
    "\n",
    "    def extract_content(self, html, url):\n",
    "        try:\n",
    "            from boilerpy3 import extractors\n",
    "            extractor = extractors.ArticleExtractor()\n",
    "            doc = extractor.get_doc(html)\n",
    "            return doc.content\n",
    "        except Exception:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for tag in soup(['script', 'style']):\n",
    "                tag.decompose()\n",
    "            return '\\n'.join(chunk.strip() for chunk in soup.get_text().splitlines() if chunk.strip())\n",
    "\n",
    "    def fetch_and_clean(self, url):\n",
    "        try:\n",
    "            if not self.is_allowed_by_robots(url):\n",
    "                return None, f\"Disallowed by robots.txt: {url}\"\n",
    "\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            if resp.status_code != 200:\n",
    "                return None, f\"HTTP error {resp.status_code} on {url}\"\n",
    "\n",
    "            html = resp.text\n",
    "            cleaned = self.extract_content(html, url)\n",
    "            return cleaned, None\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return None, f\"Request error on {url}: {str(e)}\"\n",
    "\n",
    "    def crawl(self, url, max_links=3):\n",
    "        if not url.startswith(\"http\"):\n",
    "            url = \"http://\" + url\n",
    "\n",
    "        visited = set()\n",
    "        texts = {}\n",
    "        queue = [url]\n",
    "\n",
    "        for depth in range(4):  # 2-level BFS\n",
    "            next_queue = []\n",
    "            for link in queue:\n",
    "                if link in visited:\n",
    "                    continue\n",
    "                visited.add(link)\n",
    "                text, error = self.fetch_and_clean(link)\n",
    "                if text:\n",
    "                    texts[link] = text\n",
    "                elif error:\n",
    "                    logging.warning(error)\n",
    "\n",
    "                try:\n",
    "                    html = self.session.get(link, timeout=10).text\n",
    "                    soup = BeautifulSoup(html, \"html.parser\")\n",
    "                    hrefs = [urljoin(link, tag.get(\"href\")) for tag in soup.find_all(\"a\", href=True)]\n",
    "                    hrefs = [h for h in hrefs if urlparse(h).netloc == urlparse(url).netloc]\n",
    "                    hrefs = [h for h in hrefs if all(k not in h.lower() for k in ['privacy', 'terms', 'policy'])]\n",
    "                    prioritized = [h for h in hrefs if any(p in h.lower() for p in ['about', 'service'])]\n",
    "                    remaining = [h for h in hrefs if h not in prioritized]\n",
    "                    next_queue.extend(prioritized[:max_links] + remaining[:max_links])\n",
    "                except Exception as e:\n",
    "                    logging.warning(f\"Failed to extract links from {link}: {e}\")\n",
    "            queue = next_queue\n",
    "\n",
    "        combined = \"\\n\".join(texts.values())\n",
    "        return combined if combined.strip() else None\n",
    "\n",
    "\n",
    "# DownstreamModelSingle remains the same as in training\n",
    "class DownstreamModelSingle(nn.Module):\n",
    "    def __init__(self, embed_size: int, class_num: int, common_dim: int = None):\n",
    "        super().__init__()\n",
    "        if common_dim and (common_dim != embed_size):\n",
    "            self.compress = nn.Sequential(\n",
    "                nn.Linear(embed_size, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "            )\n",
    "            final_input_dim = common_dim\n",
    "        else:\n",
    "            self.compress = nn.Identity()\n",
    "            final_input_dim = embed_size\n",
    "\n",
    "        self.fc1 = nn.Linear(final_input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.compress(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return self.fc3(out)\n",
    "\n",
    "\n",
    "class SoACerPredictor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        classifier_source=\"Shahriar/SoACer\",\n",
    "        classifier_local_dir=None,\n",
    "        device=None\n",
    "    ):\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedder_name = embedder_name\n",
    "        self.classifier_source = classifier_source\n",
    "        self.classifier_local_dir = classifier_local_dir\n",
    "        self.label_names = [\n",
    "        \"finance, marketing & human resources\",\n",
    "        \"information technology & electronics\",\n",
    "        \"consumer & supply chain\",\n",
    "        \"civil, mechanical & electrical\",\n",
    "        \"medical\",\n",
    "        \"sports, media & entertainment\",\n",
    "        \"education\",\n",
    "        \"government, defense & legal\",\n",
    "        \"travel, food & hospitality\",\n",
    "        \"non-profit\",\n",
    "        ]\n",
    "\n",
    "\n",
    "        self.embedding_model, self.tokenizer = self.load_embedder()\n",
    "        # embedding = embedding.to(self.classifier.device)\n",
    "\n",
    "        self.classifier = self.load_classifier()\n",
    "        self.classifier.eval()\n",
    "\n",
    "    def load_embedder(self):\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.embedder_name, trust_remote_code=True)\n",
    "        # model = AutoModel.from_pretrained(self.embedder_name, trust_remote_code=True)\n",
    "        # If model doesn't have a pad_token, assign the eos_token as pad_token\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "        # ----- Model Config + Loading -----\n",
    "        config_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"revision\": 'main',\n",
    "            \"use_auth_token\": None,\n",
    "            \"output_hidden_states\": True  # Enable hidden state outputs\n",
    "        }\n",
    "\n",
    "        model_config = AutoConfig.from_pretrained(self.embedder_name, **config_kwargs)\n",
    "\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "        self.embedder_name,\n",
    "        config=model_config,\n",
    "        # device_map=\"auto\",        # <--- Automatic sharding across available GPUs\n",
    "        device_map=None,  # Do not shard\n",
    "\n",
    "        # device_map = device,\n",
    "        torch_dtype=torch.float16,  # Use half precision for efficiency (optional)\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "\n",
    "        model.to(self.device)\n",
    "        return model, tokenizer\n",
    "\n",
    "    def load_classifier(self):\n",
    "        if self.classifier_local_dir:\n",
    "            config_path = os.path.join(self.classifier_local_dir, \"config.json\")\n",
    "            model_path = os.path.join(self.classifier_local_dir, \"pytorch_model.bin\")\n",
    "        else:\n",
    "            config_path = hf_hub_download(repo_id=self.classifier_source, filename=\"config.json\")\n",
    "            model_path = hf_hub_download(repo_id=self.classifier_source, filename=\"pytorch_model.bin\")\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        model = DownstreamModelSingle(\n",
    "            embed_size=config[\"embed_size\"],\n",
    "            class_num=config[\"class_num\"],\n",
    "            common_dim=config.get(\"common_dim\", config[\"embed_size\"])\n",
    "        )\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(self.device)\n",
    "        model = model.half()  # Convert weights to float16\n",
    "\n",
    "        return model\n",
    "\n",
    "    def summarize_text(self, text: str, sentences_count=12) -> str:\n",
    "        parser = PlaintextParser.from_string(text, SumyTokenizer(\"english\"))\n",
    "        summarizer = LexRankSummarizer()\n",
    "        summarizer.threshold = 0.1  # Set a custom threshold\n",
    "        summarizer.epsilon = 0.05   # Set a custom epsilon\n",
    "        summary = summarizer(parser.document, sentences_count)\n",
    "        summarized_text = ' '.join(str(sentence) for sentence in summary)\n",
    "        return summarized_text if len(summarized_text) >= 50 else \"NaN\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def embed_text(self, text: str) -> torch.Tensor:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "\n",
    "          \n",
    "        return torch.mean(outputs.hidden_states[-1], dim=1)\n",
    "\n",
    "    def predict(self, embedding: torch.Tensor, hash_key: str, url: str, summarized_text: str) -> dict:\n",
    "        with torch.no_grad():\n",
    "            logits = self.classifier(embedding)\n",
    "            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "        predicted_classes = (probs > 0.5).astype(int)\n",
    "\n",
    "        result = {\n",
    "            hash_key: {\n",
    "                \"predicted_label\": list(np.array(self.label_names)[np.where(predicted_classes == 1)[0]]),\n",
    "                \"all_probabilities\": dict(zip(self.label_names, probs.tolist())),\n",
    "                \"summaries\": summarized_text,\n",
    "                \"url\": url\n",
    "            }\n",
    "        }\n",
    "        return result\n",
    "    def __call__(self, input: str):\n",
    "        from urllib.parse import urlparse\n",
    "\n",
    "        def is_url(text):\n",
    "            return urlparse(text).scheme in [\"http\", \"https\"]\n",
    "\n",
    "        def get_domain(input_url):\n",
    "            parsed = urlparse(input_url)\n",
    "            return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "        if is_url(input):\n",
    "            crawler = SoACerCrawler()\n",
    "            domain_url = get_domain(input)\n",
    "\n",
    "            try:\n",
    "                combined_text = crawler.crawl(domain_url)\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"error\": f\"Unhandled scraping failure: {str(e)}\",\n",
    "                    \"url\": input\n",
    "                }\n",
    "\n",
    "            if not combined_text or len(combined_text.strip()) < 50:\n",
    "                return {\n",
    "                    \"error\": \"Unable to extract meaningful content from URL\",\n",
    "                    \"url\": input\n",
    "                }\n",
    "\n",
    "            summarized_text = self.summarize_text(combined_text)\n",
    "        else:\n",
    "            summarized_text = self.summarize_text(input)\n",
    "\n",
    "        if summarized_text == \"NaN\":\n",
    "            return {\n",
    "                \"error\": \"Text too short to summarize\",\n",
    "                \"url\": input\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            embedding = self.embed_text(summarized_text)\n",
    "            with torch.no_grad():\n",
    "                logits = self.classifier(embedding)\n",
    "                probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "            top_label_index = int(np.argmax(probs))\n",
    "            top_label = self.label_names[top_label_index]\n",
    "\n",
    "            return {\n",
    "                \"predicted_label\": top_label,\n",
    "                \"all_probabilities\": dict(zip(self.label_names, probs.tolist())),\n",
    "                \"summaries\": summarized_text,\n",
    "                \"url\": input\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Classification failure: {str(e)}\",\n",
    "                \"url\": input\n",
    "            }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5754931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sxs7285/anaconda3/envs/industry/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13cfc4e90c04a22ba66e3c81da0f9fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predicted_label': 'finance, marketing & human resources', 'all_probabilities': {'finance, marketing & human resources': 1.0, 'information technology & electronics': 0.0, 'consumer & supply chain': 0.0, 'civil, mechanical & electrical': 0.0, 'medical': 0.0, 'sports, media & entertainment': 0.0, 'education': 0.0, 'government, defense & legal': 0.0, 'travel, food & hospitality': 0.0, 'non-profit': 0.0}, 'summaries': 'Alesia Barnes represents the absolute best in the luxury real estate market in Hawaii. I know this from experience because she was dealing\\xa0 with a tough personal situation of her own when helping my wife and I to get our dream home, but she still took the time to help us and responded when we called her. Helped me rent a Single Family home in Kailua, HI I don’t like to do reviews, but since my realtor did an AMAZING job, this is the best i can do for her..It was SUCH a pleasure to work with Chelsey (Chelz) Doria. “Not pushy” at all, and\\xa0 really understanding our demand, we didn’t felt any sort of pressure… She was extremely patient and did her best to satisfied our needs. I will for sure request her expertise for any further purchase we might have … Bought a Townhouse home in 2013 in Kailua, HI Maikalani was a breath of fresh air for us finding our dream home. Bought a Single Family home in 2016 in Kailua, HI Destiny was very helpful and extremely responsive when searching for our perfect home. Alesia Barnes represents the absolute best in the luxury real estate market in Hawaii. He LISTENED to me – to my preferences and irritations in a home; and for it, I found the perfect place – for me. Bought a Condo home in 2017 in Downtown, Honolulu, HI My experience with Chelsey was the best I could ever dream of as a first time home buyer in a new state. She treated us like family and was so sincere in everything that she did for us. Bought a Single Family home in 2017 in Ewa Beach, HI Destiny was very helpful and extremely responsive when searching for our perfect home. He is knowledgeable about real estate market as well as renders excellent services on behalf of clients.', 'url': 'https://www.barneshawaii.com/privacy/'}\n"
     ]
    }
   ],
   "source": [
    "predictor = SoACerPredictor(\n",
    "    embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "    classifier_source=\"Shahriar/SoACer\"  # or classifier_local_dir=\"path/to/local_dir\"\n",
    ")\n",
    "\n",
    "url_to_scrape = \"https://www.interpayafrica.com/Home/Privacy\"\n",
    "url_to_scrape = \"https://www.barneshawaii.com/privacy/\"\n",
    "\n",
    "result = predictor(\n",
    "    input=url_to_scrape  \n",
    ")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bdb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"predicted_label\": \"finance, marketing & human resources\",\n",
      "  \"all_probabilities\": {\n",
      "    \"finance, marketing & human resources\": 1.0,\n",
      "    \"information technology & electronics\": 0.0,\n",
      "    \"consumer & supply chain\": 7.593631744384766e-05,\n",
      "    \"civil, mechanical & electrical\": 0.0,\n",
      "    \"medical\": 0.0,\n",
      "    \"sports, media & entertainment\": 0.0,\n",
      "    \"education\": 0.0,\n",
      "    \"government, defense & legal\": 0.0,\n",
      "    \"travel, food & hospitality\": 0.0,\n",
      "    \"non-profit\": 0.0\n",
      "  },\n",
      "  \"summaries\": \"Site Overview: Title: Interpay Content: Who can be an Interpay merchant? Account Holders What are some of the benefits of having an Interpay wallet? You can pay your fees, bills, and invoices instantly and securely anytime and anywhere You reduce the risk of using cash and of exposing your bank account when it comes to making payments You have real time access to a transaction history to help you track all your payments You can top up your Interpay wallet from anytime and from anywhere You\\u2019re the first to know when your merchant is running a sale, has some important information about member or student registration, or has a general announcement for its customers Follow this link Register to enter your registration details and to transform the way you make payments. Bank Tellers Title: InterpayAfrica Content: New User Registration Back to Home Note: Fields marked with * are mandatory. \\u2713 Valid Invalid number Title: InterpayAfrica Content: Login to Username /Email/ Mobile Talk With Us +233 26 4334390 Interpay is a payment processor that connects merchants in Ghana to local and international payment capabilities across all four mobile money platforms, five payment platforms (including Visa and MasterCard) and eleven local banks. Quick Links Title: Interpay Content: Home / About Us What is Interpay? Interpay is a electronic payment platform that facilitates traditional transactions and e-commerce Interpay facilitates the collection of payments for bills and invoices. Interpay allows merchants to receive payments from numerous customers spread out across the contry. Interpays allows merchants to easily authenticate and reconcile payments received and outstanding. Interpay can be used for disbursements. Interpay also has built in functionality for specific purpose wallets (ie funds loaded onto those wallets can only be used for specific transactions/merchants/products as determined or as required) Our PIP partners We provide platforms which help in payment collection, disbursement, and also allows easy authentication and reconciliation of payment received and outstanding. Talk With Us +233 26 4334390 Interpay is a payment processor that connects merchants in Ghana to local and international payment capabilities across all four mobile money platforms, five payment platforms (including Visa and MasterCard) and eleven local banks.\",\n",
      "  \"url\": \"Site Overview: Title: Interpay Content: Who can be an Interpay merchant? Account Holders What are some of the benefits of having an Interpay wallet? You can pay your fees, bills, and invoices instantly and securely anytime and anywhere You reduce the risk of using cash and of exposing your bank account when it comes to making payments You have real time access to a transaction history to help you track all your payments You can top up your Interpay wallet from anytime and from anywhere You\\u2019re the first to know when your merchant is running a sale, has some important information about member or student registration, or has a general announcement for its customers Follow this link Register to enter your registration details and to transform the way you make payments. Bank Tellers Title: InterpayAfrica Content: New User Registration Back to Home Note: Fields marked with * are mandatory. \\u2713 Valid Invalid number Title: InterpayAfrica Content: Login to Username /Email/ Mobile Talk With Us +233 26 4334390 Interpay is a payment processor that connects merchants in Ghana to local and international payment capabilities across all four mobile money platforms, five payment platforms (including Visa and MasterCard) and eleven local banks. Quick Links Title: Interpay Content: Home / About Us What is Interpay? Interpay is a electronic payment platform that facilitates traditional transactions and e-commerce Interpay facilitates the collection of payments for bills and invoices. Interpay allows merchants to receive payments from numerous customers spread out across the contry. Interpays allows merchants to easily authenticate and reconcile payments received and outstanding. Interpay can be used for disbursements. Interpay also has built in functionality for specific purpose wallets (ie funds loaded onto those wallets can only be used for specific transactions/merchants/products as determined or as required) Our PIP partners We provide platforms which help in payment collection, disbursement, and also allows easy authentication and reconciliation of payment received and outstanding. Talk With Us +233 26 4334390 Interpay is a payment processor that connects merchants in Ghana to local and international payment capabilities across all four mobile money platforms, five payment platforms (including Visa and MasterCard) and eleven local banks. Quick Links\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load your dataset from Hugging Face\n",
    "dataset = load_dataset(\"Shahriar/SoAC_Corpus\", split=\"train\")\n",
    "\n",
    "# Pick the first sample summary\n",
    "text = dataset[0][\"Website_Summary\"]\n",
    "\n",
    "# # Run your predictor\n",
    "# predictor = SoACerPredictor(\n",
    "#     embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "#     classifier_source=\"Shahriar/SoACer\"  # or classifier_local_dir=\"path/to/local_dir\"\n",
    "# )\n",
    "result = predictor(text)\n",
    "\n",
    "# Print output nicely\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019319e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hash-key from the pipeline\n",
    "# check the prediction logic\n",
    "# make it executable with command line args\n",
    "# add a simple CLI interface to run the predictor\n",
    "# add it as a tool to Langchain and AutoGen\n",
    "# look at the lexrank summarizer implementation\n",
    "# can i remove the gpu memory before fedding it into linear head\n",
    "\n",
    "# you don;t scrap exactly like scrapy\n",
    "\n",
    "# add await and stuff\n",
    "# add batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8de8893",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m      2\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "def embed_text(self, text: str) -> torch.Tensor:\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd0c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "from boilerpy3 import extractors\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class AsyncSoACerCrawler:\n",
    "    \"\"\"\n",
    "    Asynchronous web crawler that:\n",
    "      - Respects robots.txt\n",
    "      - Performs a 2-level BFS within the same domain\n",
    "      - Extracts and cleans text via boilerpy3 or BeautifulSoup\n",
    "    \"\"\"\n",
    "    def __init__(self, user_agent=\"SoACerBot\", max_links=3, concurrency=10):\n",
    "        self.user_agent = user_agent\n",
    "        self.max_links = max_links\n",
    "        # Semaphore to limit concurrent HTTP requests\n",
    "        self._http_semaphore = asyncio.Semaphore(concurrency)\n",
    "        # Cache robots.txt parsers per domain\n",
    "        self._robots_cache = {}\n",
    "        # aiohttp session (initialized on first use)\n",
    "        self._session = None\n",
    "\n",
    "    async def _get_session(self):\n",
    "        if self._session is None or self._session.closed:\n",
    "            self._session = aiohttp.ClientSession(headers={\"User-Agent\": self.user_agent})\n",
    "        return self._session\n",
    "\n",
    "    async def _fetch_html(self, url: str) -> (str, str):\n",
    "        \"\"\"\n",
    "        Fetch HTML content for a given URL, respecting a semaphore to limit concurrency.\n",
    "        Returns (html_text, error_msg). If error_msg is not None, html_text will be None.\n",
    "        \"\"\"\n",
    "        session = await self._get_session()\n",
    "\n",
    "        async with self._http_semaphore:\n",
    "            try:\n",
    "                async with session.get(url, timeout=10) as resp:\n",
    "                    if resp.status != 200:\n",
    "                        return None, f\"HTTP error {resp.status} on {url}\"\n",
    "                    text = await resp.text()\n",
    "                    return text, None\n",
    "            except asyncio.TimeoutError:\n",
    "                return None, f\"Timeout fetching {url}\"\n",
    "            except aiohttp.ClientError as e:\n",
    "                return None, f\"Request error on {url}: {str(e)}\"\n",
    "            except Exception as e:\n",
    "                return None, f\"Unexpected error on {url}: {str(e)}\"\n",
    "\n",
    "    async def _allowed_by_robots(self, url: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check robots.txt for the domain of 'url'. Fetch and parse robots.txt if not cached.\n",
    "        \"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        domain = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        rp = self._robots_cache.get(domain)\n",
    "\n",
    "        if rp is None:\n",
    "            rp = RobotFileParser()\n",
    "            robots_url = f\"{domain}/robots.txt\"\n",
    "            session = await self._get_session()\n",
    "            try:\n",
    "                async with session.get(robots_url, timeout=10) as resp:\n",
    "                    if resp.status == 200:\n",
    "                        text = await resp.text()\n",
    "                        rp.parse(text.splitlines())\n",
    "                    else:\n",
    "                        # If no robots.txt or inaccessible, allow by default\n",
    "                        rp.allow_all = True\n",
    "            except Exception:\n",
    "                rp.allow_all = True\n",
    "            self._robots_cache[domain] = rp\n",
    "\n",
    "        return rp.can_fetch(self.user_agent, url)\n",
    "\n",
    "    def _extract_content(self, html: str, url: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract and clean main text from HTML using boilerpy3. If that fails, fallback to BeautifulSoup.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            extractor = extractors.ArticleExtractor()\n",
    "            doc = extractor.get_doc(html)\n",
    "            return doc.content\n",
    "        except Exception:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for tag in soup(['script', 'style']):\n",
    "                tag.decompose()\n",
    "            text_chunks = [\n",
    "                chunk.strip()\n",
    "                for chunk in soup.get_text().splitlines()\n",
    "                if chunk.strip()\n",
    "            ]\n",
    "            return \"\\n\".join(text_chunks)\n",
    "\n",
    "    async def _crawl_one(self, url: str) -> (str, list, str):\n",
    "        \"\"\"\n",
    "        Crawl a single URL:\n",
    "          - Check robots.txt\n",
    "          - Fetch and clean content\n",
    "          - Extract up to max_links child URLs within same domain\n",
    "        Returns (cleaned_text, link_list, error_msg).\n",
    "        \"\"\"\n",
    "        # Check robots.txt\n",
    "        allowed = await self._allowed_by_robots(url)\n",
    "        if not allowed:\n",
    "            return None, [], f\"Disallowed by robots.txt: {url}\"\n",
    "\n",
    "        # Fetch HTML\n",
    "        html, error = await self._fetch_html(url)\n",
    "        if error:\n",
    "            return None, [], error\n",
    "\n",
    "        # Clean text\n",
    "        cleaned = self._extract_content(html, url)\n",
    "\n",
    "        # Extract child links within same domain, excluding privacy/terms/policy pages\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        parsed_seed = urlparse(url)\n",
    "        domain_netloc = parsed_seed.netloc\n",
    "\n",
    "        hrefs = []\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href = urljoin(url, tag.get(\"href\"))\n",
    "            parsed_href = urlparse(href)\n",
    "            if parsed_href.netloc != domain_netloc:\n",
    "                continue\n",
    "            lower_href = href.lower()\n",
    "            if any(keyword in lower_href for keyword in ['privacy', 'terms', 'policy']):\n",
    "                continue\n",
    "            hrefs.append(href)\n",
    "\n",
    "        # Prioritize links containing 'about' or 'service'\n",
    "        prioritized = [h for h in hrefs if any(p in h.lower() for p in ['about', 'service'])]\n",
    "        remaining = [h for h in hrefs if h not in prioritized]\n",
    "\n",
    "        # Take up to max_links from each list\n",
    "        child_links = prioritized[: self.max_links] + remaining[: self.max_links]\n",
    "        return cleaned, child_links, None\n",
    "\n",
    "    async def crawl(self, seed_url: str) -> str:\n",
    "        \"\"\"\n",
    "        Perform a 2-level BFS crawl starting from seed_url. Return concatenated text from all visited pages.\n",
    "        \"\"\"\n",
    "        if not seed_url.startswith(\"http\"):\n",
    "            seed_url = \"http://\" + seed_url\n",
    "\n",
    "        visited = set()\n",
    "        texts = {}\n",
    "        queue = [seed_url]\n",
    "\n",
    "        # Up to 2 “hops” from the seed: depth 0 → 3 (inclusive)\n",
    "        for depth in range(4):\n",
    "            tasks = []\n",
    "            for link in queue:\n",
    "                if link in visited:\n",
    "                    continue\n",
    "                visited.add(link)\n",
    "                tasks.append(asyncio.create_task(self._crawl_one(link)))\n",
    "\n",
    "            if not tasks:\n",
    "                break\n",
    "\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            next_queue = []\n",
    "\n",
    "            for idx, result in enumerate(results):\n",
    "                if isinstance(result, Exception):\n",
    "                    # Unexpected exception in _crawl_one\n",
    "                    logging.warning(f\"Exception crawling {queue[idx]}: {str(result)}\")\n",
    "                    continue\n",
    "\n",
    "                cleaned, child_links, error = result\n",
    "                current_url = queue[idx]\n",
    "\n",
    "                if cleaned:\n",
    "                    texts[current_url] = cleaned\n",
    "                elif error:\n",
    "                    logging.warning(error)\n",
    "\n",
    "                # Enqueue child links for next depth\n",
    "                for child in child_links:\n",
    "                    if child not in visited:\n",
    "                        next_queue.append(child)\n",
    "\n",
    "            queue = next_queue\n",
    "\n",
    "        # Close session when done\n",
    "        if self._session and not self._session.closed:\n",
    "            await self._session.close()\n",
    "\n",
    "        # Return combined text\n",
    "        combined = \"\\n\".join(texts.values())\n",
    "        return combined if combined.strip() else None\n",
    "\n",
    "\n",
    "class DownstreamModelSingle(nn.Module):\n",
    "    \"\"\"\n",
    "    The same classifier architecture used in training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size: int, class_num: int, common_dim: int = None):\n",
    "        super().__init__()\n",
    "        if common_dim and (common_dim != embed_size):\n",
    "            self.compress = nn.Sequential(\n",
    "                nn.Linear(embed_size, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "            )\n",
    "            final_input_dim = common_dim\n",
    "        else:\n",
    "            self.compress = nn.Identity()\n",
    "            final_input_dim = embed_size\n",
    "\n",
    "        self.fc1 = nn.Linear(final_input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.compress(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return self.fc3(out)\n",
    "\n",
    "\n",
    "class SoACerPredictor:\n",
    "    \"\"\"\n",
    "    Predictor that wraps:\n",
    "      - AsyncSoACerCrawler for asynchronous crawling\n",
    "      - Summarization via Sumy (blocking, offloaded to executor)\n",
    "      - Embedding + classification via HuggingFace (blocking, offloaded to executor)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        classifier_source=\"Shahriar/SoACer\",\n",
    "        classifier_local_dir=None,\n",
    "        device=None,\n",
    "        crawler_max_links=3,\n",
    "        crawler_concurrency=10,\n",
    "    ):\n",
    "        # Device setup for PyTorch\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Label names (must match classifier training)\n",
    "        self.label_names = [\n",
    "            \"finance, marketing & human resources\",\n",
    "            \"information technology & electronics\",\n",
    "            \"consumer & supply chain\",\n",
    "            \"civil, mechanical & electrical\",\n",
    "            \"medical\",\n",
    "            \"sports, media & entertainment\",\n",
    "            \"education\",\n",
    "            \"government, defense & legal\",\n",
    "            \"travel, food & hospitality\",\n",
    "            \"non-profit\",\n",
    "        ]\n",
    "\n",
    "        # Initialize the asynchronous crawler\n",
    "        self.async_crawler = AsyncSoACerCrawler(\n",
    "            user_agent=\"SoACerBot\",\n",
    "            max_links=crawler_max_links,\n",
    "            concurrency=crawler_concurrency,\n",
    "        )\n",
    "\n",
    "        # Load embedder + classifier\n",
    "        self.embedder_name = embedder_name\n",
    "        self.classifier_source = classifier_source\n",
    "        self.classifier_local_dir = classifier_local_dir\n",
    "\n",
    "        self.embedding_model, self.tokenizer = self._load_embedder()\n",
    "        self.classifier = self._load_classifier()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        # Get event loop and (optionally) a dedicated ThreadPoolExecutor\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "\n",
    "    def _load_embedder(self):\n",
    "        \"\"\"\n",
    "        Load tokenizer and causal LM for embeddings. Returns (model, tokenizer).\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.embedder_name, trust_remote_code=True)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "\n",
    "        config_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"revision\": \"main\",\n",
    "            \"use_auth_token\": None,\n",
    "            \"output_hidden_states\": True,\n",
    "        }\n",
    "        model_config = AutoConfig.from_pretrained(self.embedder_name, **config_kwargs)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.embedder_name,\n",
    "            config=model_config,\n",
    "            device_map=None,            # No automatic sharding\n",
    "            torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "            attn_implementation=\"eager\",\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        return model, tokenizer\n",
    "\n",
    "    def _load_classifier(self):\n",
    "        \"\"\"\n",
    "        Load the downstream classifier from either a local directory or HuggingFace Hub.\n",
    "        \"\"\"\n",
    "        if self.classifier_local_dir:\n",
    "            config_path = os.path.join(self.classifier_local_dir, \"config.json\")\n",
    "            model_path = os.path.join(self.classifier_local_dir, \"pytorch_model.bin\")\n",
    "        else:\n",
    "            config_path = hf_hub_download(repo_id=self.classifier_source, filename=\"config.json\")\n",
    "            model_path = hf_hub_download(repo_id=self.classifier_source, filename=\"pytorch_model.bin\")\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        model = DownstreamModelSingle(\n",
    "            embed_size=config[\"embed_size\"],\n",
    "            class_num=config[\"class_num\"],\n",
    "            common_dim=config.get(\"common_dim\", config[\"embed_size\"]),\n",
    "        )\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(self.device)\n",
    "        model = model.half()  # Convert weights to float16\n",
    "        return model\n",
    "\n",
    "    def summarize_text(self, text: str, sentences_count=12) -> str:\n",
    "        \"\"\"\n",
    "        Synchronous summarization using Sumy (LexRank).\n",
    "        \"\"\"\n",
    "        parser = PlaintextParser.from_string(text, SumyTokenizer(\"english\"))\n",
    "        summarizer = LexRankSummarizer()\n",
    "        summarizer.threshold = 0.1\n",
    "        summarizer.epsilon = 0.05\n",
    "        summary = summarizer(parser.document, sentences_count)\n",
    "        summarized_text = \" \".join(str(sentence) for sentence in summary)\n",
    "        return summarized_text if len(summarized_text) >= 50 else \"NaN\"\n",
    "\n",
    "    def embed_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Synchronous embedding: tokenize, run through the LM, and mean-pool the last hidden state.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "        # Mean-pool over sequence dimension\n",
    "        return torch.mean(outputs.hidden_states[-1], dim=1)\n",
    "\n",
    "    def _classify_from_embedding(self, embedding: torch.Tensor, url: str, summarized_text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Synchronous classification from an embedding. Returns the result dict.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.classifier(embedding)\n",
    "            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "        top_label_index = int(np.argmax(probs))\n",
    "        top_label = self.label_names[top_label_index]\n",
    "        return {\n",
    "            \"predicted_label\": top_label,\n",
    "            \"all_probabilities\": dict(zip(self.label_names, probs.tolist())),\n",
    "            \"summaries\": summarized_text,\n",
    "            \"url\": url,\n",
    "        }\n",
    "\n",
    "    async def __call_async__(self, input_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Asynchronous pipeline:\n",
    "          1. If input_str is a URL: crawl domain (async)\n",
    "          2. Summarize text (blocking → run_in_executor)\n",
    "          3. Embed text (blocking → run_in_executor)\n",
    "          4. Classify embedding (blocking → run_in_executor)\n",
    "        Returns a dict with keys: predicted_label, all_probabilities, summaries, url OR error.\n",
    "        \"\"\"\n",
    "        # Helper to detect URLs\n",
    "        def is_url(text: str) -> bool:\n",
    "            parsed = urlparse(text)\n",
    "            return parsed.scheme in (\"http\", \"https\")\n",
    "\n",
    "        def get_domain(input_url: str) -> str:\n",
    "            parsed = urlparse(input_url)\n",
    "            return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "        # 1. Crawl if URL, else treat input as raw text\n",
    "        if is_url(input_str):\n",
    "            domain_url = get_domain(input_str)\n",
    "            try:\n",
    "                combined_text = await self.async_crawler.crawl(domain_url)\n",
    "            except Exception as e:\n",
    "                return {\"error\": f\"Unhandled scraping failure: {str(e)}\", \"url\": input_str}\n",
    "\n",
    "            if not combined_text or len(combined_text.strip()) < 50:\n",
    "                return {\"error\": \"Unable to extract meaningful content from URL\", \"url\": input_str}\n",
    "        else:\n",
    "            combined_text = input_str\n",
    "\n",
    "        # 2. Summarize (offload to executor to avoid blocking the event loop)\n",
    "        summarized_text = await self.loop.run_in_executor(None, self.summarize_text, combined_text)\n",
    "        if summarized_text == \"NaN\":\n",
    "            return {\"error\": \"Text too short to summarize\", \"url\": input_str}\n",
    "\n",
    "        # 3. Embed (offload to executor)\n",
    "        try:\n",
    "            embedding = await self.loop.run_in_executor(None, self.embed_text, summarized_text)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Embedding failure: {str(e)}\", \"url\": input_str}\n",
    "\n",
    "        # 4. Classify (offload to executor)\n",
    "        try:\n",
    "            result = await self.loop.run_in_executor(\n",
    "                None, self._classify_from_embedding, embedding, input_str, summarized_text\n",
    "            )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Classification failure: {str(e)}\", \"url\": input_str}\n",
    "\n",
    "    def __call__(self, input_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Synchronous wrapper that runs the async pipeline to completion.\n",
    "        \"\"\"\n",
    "        return self.loop.run_until_complete(self.__call_async__(input_str))\n",
    "\n",
    "\n",
    "# Example usage (synchronous):\n",
    "# predictor = SoACerPredictor(\n",
    "#     embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "#     classifier_source=\"Shahriar/SoACer\",\n",
    "#     classifier_local_dir=None,\n",
    "#     device=None,\n",
    "# )\n",
    "# result = predictor(\"https://example.com\")\n",
    "# print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527466aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sxs7285/anaconda3/envs/industry/lib/python3.8/site-packages/transformers/generation/configuration_utils.py:777: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b620c044b88429486f2d2f8f26a6c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:HTTP error 403 on https://www.visitmyrtlebeach.com\n",
      "WARNING:root:Request error on https://www.barneshawaii.com: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2776)\n",
      "WARNING:root:Request error on https://www.copra.org: Connector is closed.\n",
      "WARNING:root:Request error on https://www.greyorange.com: [Errno 1] [SSL: APPLICATION_DATA_AFTER_CLOSE_NOTIFY] application data after close notify (_ssl.c:2776)\n",
      "WARNING:root:Request error on https://bnrmediagroup.ca: Connector is closed.\n",
      "WARNING:root:Request error on https://www.interpayafrica.com: Connector is closed.\n",
      "WARNING:root:Request error on https://secure.retireware.com: Connector is closed.\n",
      "WARNING:root:Request error on https://bhogalpartners.co.uk: Connector is closed.\n",
      "WARNING:root:Request error on https://www.cityalliance.com.au: Connector is closed.\n",
      "WARNING:root:Request error on https://bizfacility.com: Connector is closed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Result for URL #1: https://www.interpayafrica.com/Home/Privacy ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://www.interpayafrica.com/Home/Privacy\"\n",
      "}\n",
      "\n",
      "=== Result for URL #2: https://bizfacility.com/privacy-policy/ ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://bizfacility.com/privacy-policy/\"\n",
      "}\n",
      "\n",
      "=== Result for URL #3: https://www.barneshawaii.com/privacy/ ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://www.barneshawaii.com/privacy/\"\n",
      "}\n",
      "\n",
      "=== Result for URL #4: https://bhogalpartners.co.uk/privacy-policy/ ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://bhogalpartners.co.uk/privacy-policy/\"\n",
      "}\n",
      "\n",
      "=== Result for URL #5: https://bnrmediagroup.ca/privacy-policy/ ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://bnrmediagroup.ca/privacy-policy/\"\n",
      "}\n",
      "\n",
      "=== Result for URL #6: https://www.copra.org/privacy-policy ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://www.copra.org/privacy-policy\"\n",
      "}\n",
      "\n",
      "=== Result for URL #7: https://www.visitmyrtlebeach.com/privacy-policy ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://www.visitmyrtlebeach.com/privacy-policy\"\n",
      "}\n",
      "\n",
      "=== Result for URL #8: https://secure.retireware.com/privacy.aspx ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://secure.retireware.com/privacy.aspx\"\n",
      "}\n",
      "\n",
      "=== Result for URL #9: https://www.cityalliance.com.au/privacy-policy-disclaimer/ ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://www.cityalliance.com.au/privacy-policy-disclaimer/\"\n",
      "}\n",
      "\n",
      "=== Result for URL #10: https://www.greyorange.com/privacy-policy/ ===\n",
      "{\n",
      "  \"error\": \"Unable to extract meaningful content from URL\",\n",
      "  \"url\": \"https://www.greyorange.com/privacy-policy/\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Assume SoACerPredictor and classify_multiple_websites have already been defined/imported\n",
    "# from the code you added previously.\n",
    "\n",
    "async def classify_multiple_websites(url_list):\n",
    "    \"\"\"\n",
    "    Given a list of URLs, run SoACerPredictor.__call_async__ on each concurrently.\n",
    "    Returns a list of result dicts (in the same order as url_list).\n",
    "    \"\"\"\n",
    "    predictor = SoACerPredictor(\n",
    "        embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        classifier_source=\"Shahriar/SoACer\",\n",
    "        classifier_local_dir=None,\n",
    "        device=None,\n",
    "        crawler_max_links=3,\n",
    "        crawler_concurrency=10,\n",
    "    )\n",
    "\n",
    "    tasks = [asyncio.create_task(predictor.__call_async__(url)) for url in url_list]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=False)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # # 1. Load the Hugging Face dataset\n",
    "    # dataset = load_dataset(\"Shahriar/SoAC_Corpus\", split=\"train\")\n",
    "\n",
    "    # # 2. Extract the first 10 Privacy_Policy_URL values\n",
    "    # urls = [dataset[i][\"Privacy_Policy_URL\"] for i in range(10)]\n",
    "\n",
    "    # # 3. Run the async classification on those 10 URLs\n",
    "    # all_results = asyncio.run(classify_multiple_websites(urls))\n",
    "\n",
    "    # # 4. Pretty-print each result\n",
    "    # for idx, (url, res) in enumerate(zip(urls, all_results), start=1):\n",
    "    #     print(f\"\\n=== Result for URL #{idx}: {url} ===\")\n",
    "    #     print(json.dumps(res, indent=2))\n",
    "\n",
    "    # 1. Load the Hugging Face dataset\n",
    "    from datasets import load_dataset\n",
    "    dataset = load_dataset(\"Shahriar/SoAC_Corpus\", split=\"train\")\n",
    "\n",
    "    # 2. Extract the first 10 Privacy_Policy_URL values\n",
    "    urls = [dataset[i][\"Privacy_Policy_URL\"] for i in range(10)]\n",
    "\n",
    "    # 3. Call the async function with `await` directly\n",
    "    all_results = await classify_multiple_websites(urls)\n",
    "\n",
    "    # 4. Pretty-print each result\n",
    "    import json\n",
    "    for idx, (url, res) in enumerate(zip(urls, all_results), start=1):\n",
    "        print(f\"\\n=== Result for URL #{idx}: {url} ===\")\n",
    "        print(json.dumps(res, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9928fc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cb4d4ee07564241865dd1141f612539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "import logging\n",
    "\n",
    "from boilerpy3 import extractors\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer as SumyTokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import logging\n",
    "from urllib.parse import urlparse, urljoin\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from bs4 import BeautifulSoup\n",
    "from boilerpy3 import extractors\n",
    "\n",
    "class AsyncSoACerCrawler:\n",
    "    \"\"\"\n",
    "    Robust asynchronous web crawler with:\n",
    "      - Per-domain aiohttp session\n",
    "      - Resilient to TLS teardown / anti-bot protection\n",
    "      - Retry logic, optional link BFS\n",
    "    \"\"\"\n",
    "    def __init__(self, user_agent=None, max_links=3, concurrency=3, retries=2, crawl_delay=0.3):\n",
    "        self.user_agent = user_agent or (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/114.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "        self.max_links = max_links\n",
    "        self.concurrency = concurrency\n",
    "        self.retries = retries\n",
    "        self.crawl_delay = crawl_delay\n",
    "        self._robots_cache = {}\n",
    "\n",
    "    async def _fetch_html(self, url: str, session: aiohttp.ClientSession) -> (str, str):\n",
    "        \"\"\"\n",
    "        Attempt to fetch HTML with retry and exponential backoff.\n",
    "        Returns (html, error_message).\n",
    "        \"\"\"\n",
    "        for attempt in range(self.retries + 1):\n",
    "            try:\n",
    "                async with session.get(url, timeout=60) as resp:\n",
    "                    if resp.status != 200:\n",
    "                        return None, f\"HTTP error {resp.status} on {url}\"\n",
    "                    return await resp.text(), None\n",
    "            except (aiohttp.ClientError, asyncio.TimeoutError) as e:\n",
    "                if attempt < self.retries:\n",
    "                    await asyncio.sleep(2 ** attempt * 0.5)\n",
    "                    continue\n",
    "                return None, f\"Request error on {url}: {str(e)}\"\n",
    "            except Exception as e:\n",
    "                return None, f\"Unexpected error on {url}: {str(e)}\"\n",
    "\n",
    "    async def _allowed_by_robots(self, url: str, session: aiohttp.ClientSession) -> bool:\n",
    "        parsed = urlparse(url)\n",
    "        domain = f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "        rp = self._robots_cache.get(domain)\n",
    "\n",
    "        if rp is None:\n",
    "            rp = RobotFileParser()\n",
    "            robots_url = f\"{domain}/robots.txt\"\n",
    "            try:\n",
    "                async with session.get(robots_url, timeout=60) as resp:\n",
    "                    if resp.status == 200:\n",
    "                        text = await resp.text()\n",
    "                        rp.parse(text.splitlines())\n",
    "                    else:\n",
    "                        rp.allow_all = True\n",
    "            except Exception:\n",
    "                rp.allow_all = True\n",
    "            self._robots_cache[domain] = rp\n",
    "\n",
    "        can_fetch = rp.can_fetch(self.user_agent, url)\n",
    "        if not can_fetch:\n",
    "            logging.warning(f\"robots.txt disallows fetching: {url}\")\n",
    "        return True  # Always try, just log warning\n",
    "\n",
    "    def _extract_content(self, html: str, url: str) -> str:\n",
    "        try:\n",
    "            extractor = extractors.ArticleExtractor()\n",
    "            return extractor.get_doc(html).content\n",
    "        except Exception:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            for tag in soup(['script', 'style']):\n",
    "                tag.decompose()\n",
    "            lines = [line.strip() for line in soup.get_text().splitlines() if line.strip()]\n",
    "            return \"\\n\".join(lines)\n",
    "\n",
    "    async def _crawl_one(self, url: str, session: aiohttp.ClientSession) -> (str, list, str):\n",
    "        await self._allowed_by_robots(url, session)\n",
    "        html, error = await self._fetch_html(url, session)\n",
    "        if error:\n",
    "            return None, [], error\n",
    "\n",
    "        cleaned = self._extract_content(html, url)\n",
    "\n",
    "        # Skip BFS if disabled\n",
    "        if self.max_links == 0:\n",
    "            return cleaned, [], None\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        domain_netloc = urlparse(url).netloc\n",
    "        hrefs = []\n",
    "\n",
    "        for tag in soup.find_all(\"a\", href=True):\n",
    "            href = urljoin(url, tag.get(\"href\"))\n",
    "            if urlparse(href).netloc != domain_netloc:\n",
    "                continue\n",
    "            if any(k in href.lower() for k in ['privacy', 'terms', 'policy']):\n",
    "                continue\n",
    "            hrefs.append(href)\n",
    "\n",
    "        prioritized = [h for h in hrefs if any(p in h.lower() for p in ['about', 'service'])]\n",
    "        remaining = [h for h in hrefs if h not in prioritized]\n",
    "        child_links = prioritized[:self.max_links] + remaining[:self.max_links]\n",
    "\n",
    "        return cleaned, child_links, None\n",
    "\n",
    "    async def crawl(self, seed_url: str) -> str:\n",
    "        \"\"\"\n",
    "        Perform a 2-level BFS starting from the seed URL.\n",
    "        Returns combined text or None.\n",
    "        \"\"\"\n",
    "        if not seed_url.startswith(\"http\"):\n",
    "            seed_url = \"http://\" + seed_url\n",
    "\n",
    "        visited = set()\n",
    "        texts = {}\n",
    "        queue = [seed_url]\n",
    "\n",
    "        async with aiohttp.ClientSession(headers={\"User-Agent\": self.user_agent}) as session:\n",
    "            for depth in range(4):\n",
    "                tasks = []\n",
    "                for link in queue:\n",
    "                    if link in visited:\n",
    "                        continue\n",
    "                    visited.add(link)\n",
    "                    tasks.append(self._crawl_one(link, session))\n",
    "\n",
    "                if not tasks:\n",
    "                    break\n",
    "\n",
    "                results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "                next_queue = []\n",
    "\n",
    "                for i, result in enumerate(results):\n",
    "                    if isinstance(result, Exception):\n",
    "                        logging.warning(f\"Exception crawling {queue[i]}: {str(result)}\")\n",
    "                        continue\n",
    "\n",
    "                    cleaned, child_links, error = result\n",
    "                    if cleaned:\n",
    "                        texts[queue[i]] = cleaned\n",
    "                    elif error:\n",
    "                        logging.warning(error)\n",
    "\n",
    "                    next_queue.extend(child for child in child_links if child not in visited)\n",
    "\n",
    "                queue = next_queue\n",
    "                await asyncio.sleep(self.crawl_delay)\n",
    "\n",
    "        combined = \"\\n\".join(texts.values())\n",
    "        return combined if combined.strip() else None\n",
    "\n",
    "\n",
    "\n",
    "class DownstreamModelSingle(nn.Module):\n",
    "    \"\"\"\n",
    "    The same classifier architecture used in training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size: int, class_num: int, common_dim: int = None):\n",
    "        super().__init__()\n",
    "        if common_dim and (common_dim != embed_size):\n",
    "            self.compress = nn.Sequential(\n",
    "                nn.Linear(embed_size, common_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3),\n",
    "            )\n",
    "            final_input_dim = common_dim\n",
    "        else:\n",
    "            self.compress = nn.Identity()\n",
    "            final_input_dim = embed_size\n",
    "\n",
    "        self.fc1 = nn.Linear(final_input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.act1 = nn.LeakyReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.act2 = nn.LeakyReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc3 = nn.Linear(256, class_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.compress(x)\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dropout2(out)\n",
    "        return self.fc3(out)\n",
    "\n",
    "\n",
    "class SoACerPredictor:\n",
    "    \"\"\"\n",
    "    Predictor that wraps:\n",
    "      - AsyncSoACerCrawler for async crawling\n",
    "      - Summarization via Sumy (blocking, offloaded to executor)\n",
    "      - Embedding + classification via HuggingFace (blocking, offloaded)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        classifier_source=\"Shahriar/SoACer\",\n",
    "        classifier_local_dir=None,\n",
    "        device=None,\n",
    "        crawler_max_links=3,\n",
    "        crawler_concurrency=10,\n",
    "    ):\n",
    "        # PyTorch device\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Label names (must match the classifier training)\n",
    "        self.label_names = [\n",
    "            \"finance, marketing & human resources\",\n",
    "            \"information technology & electronics\",\n",
    "            \"consumer & supply chain\",\n",
    "            \"civil, mechanical & electrical\",\n",
    "            \"medical\",\n",
    "            \"sports, media & entertainment\",\n",
    "            \"education\",\n",
    "            \"government, defense & legal\",\n",
    "            \"travel, food & hospitality\",\n",
    "            \"non-profit\",\n",
    "        ]\n",
    "\n",
    "        # Async crawler\n",
    "        self.async_crawler = AsyncSoACerCrawler(\n",
    "            user_agent=None,  # uses realistic browser UA by default\n",
    "            max_links=crawler_max_links,\n",
    "            concurrency=crawler_concurrency,\n",
    "        )\n",
    "\n",
    "        # Load embedder + tokenizer\n",
    "        self.embedder_name = embedder_name\n",
    "        self.classifier_source = classifier_source\n",
    "        self.classifier_local_dir = classifier_local_dir\n",
    "\n",
    "        self.embedding_model, self.tokenizer = self._load_embedder()\n",
    "        self.classifier = self._load_classifier()\n",
    "        self.classifier.eval()\n",
    "\n",
    "        # Use the existing event loop\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "\n",
    "    def _load_embedder(self):\n",
    "        \"\"\"\n",
    "        Load tokenizer and causal LM for embeddings. Returns (model, tokenizer).\n",
    "        \"\"\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.embedder_name, trust_remote_code=True)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"right\"\n",
    "\n",
    "        config_kwargs = {\n",
    "            \"trust_remote_code\": True,\n",
    "            \"revision\": \"main\",\n",
    "            \"use_auth_token\": None,\n",
    "            \"output_hidden_states\": True,\n",
    "        }\n",
    "        model_config = AutoConfig.from_pretrained(self.embedder_name, **config_kwargs)\n",
    "\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.embedder_name,\n",
    "            config=model_config,\n",
    "            device_map=None,            # No automatic sharding\n",
    "            torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "            attn_implementation=\"eager\",\n",
    "        )\n",
    "        model.to(self.device)\n",
    "        return model, tokenizer\n",
    "\n",
    "    def _load_classifier(self):\n",
    "        \"\"\"\n",
    "        Load the downstream classifier from either a local directory or HuggingFace Hub.\n",
    "        \"\"\"\n",
    "        if self.classifier_local_dir:\n",
    "            config_path = os.path.join(self.classifier_local_dir, \"config.json\")\n",
    "            model_path = os.path.join(self.classifier_local_dir, \"pytorch_model.bin\")\n",
    "        else:\n",
    "            config_path = hf_hub_download(repo_id=self.classifier_source, filename=\"config.json\")\n",
    "            model_path = hf_hub_download(repo_id=self.classifier_source, filename=\"pytorch_model.bin\")\n",
    "\n",
    "        with open(config_path) as f:\n",
    "            config = json.load(f)\n",
    "\n",
    "        model = DownstreamModelSingle(\n",
    "            embed_size=config[\"embed_size\"],\n",
    "            class_num=config[\"class_num\"],\n",
    "            common_dim=config.get(\"common_dim\", config[\"embed_size\"]),\n",
    "        )\n",
    "        state_dict = torch.load(model_path, map_location=self.device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(self.device)\n",
    "        model = model.half()  # Convert weights to float16\n",
    "        return model\n",
    "\n",
    "    def summarize_text(self, text: str, sentences_count=12) -> str:\n",
    "        \"\"\"\n",
    "        Synchronous summarization with Sumy (LexRank).\n",
    "        If fewer than 50 chars, returns \"NaN\".\n",
    "        \"\"\"\n",
    "        parser = PlaintextParser.from_string(text, SumyTokenizer(\"english\"))\n",
    "        summarizer = LexRankSummarizer()\n",
    "        summarizer.threshold = 0.1\n",
    "        summarizer.epsilon = 0.05\n",
    "        summary = summarizer(parser.document, sentences_count)\n",
    "        summarized_text = \" \".join(str(sentence) for sentence in summary)\n",
    "        return summarized_text if len(summarized_text) >= 50 else \"NaN\"\n",
    "\n",
    "    def embed_text(self, text: str) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Synchronous embedding: tokenize, run through LM, and mean-pool last hidden state.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "        return torch.mean(outputs.hidden_states[-1], dim=1)\n",
    "\n",
    "    def _classify_from_embedding(self, embedding: torch.Tensor, url: str, final_text: str) -> dict:\n",
    "        \"\"\"\n",
    "        Synchronous classification from an embedding. Returns the result dict.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.classifier(embedding)\n",
    "            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n",
    "\n",
    "        top_label_index = int(np.argmax(probs))\n",
    "        top_label = self.label_names[top_label_index]\n",
    "        return {\n",
    "            \"predicted_label\": top_label,\n",
    "            \"all_probabilities\": dict(zip(self.label_names, probs.tolist())),\n",
    "            \"summaries\": final_text,\n",
    "            \"url\": url,\n",
    "        }\n",
    "\n",
    "    async def __call_async__(self, input_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Asynchronous pipeline:\n",
    "          1. Crawl if input_str is a URL\n",
    "          2. Summarize (offloaded to executor)\n",
    "          3. Embed text (offloaded)\n",
    "          4. Classify embedding (offloaded)\n",
    "        Returns a dict with predicted_label, all_probabilities, summaries, url or an error.\n",
    "        \"\"\"\n",
    "        # Detect if input_str is a URL\n",
    "        def is_url(text: str) -> bool:\n",
    "            parsed = urlparse(text)\n",
    "            return parsed.scheme in (\"http\", \"https\")\n",
    "\n",
    "        def get_domain(input_url: str) -> str:\n",
    "            parsed = urlparse(input_url)\n",
    "            return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "        # 1. Crawl if it's a URL\n",
    "        if is_url(input_str):\n",
    "            domain_url = get_domain(input_str)\n",
    "            print(f\"Starting crawl for: {domain_url}\")\n",
    "            try:\n",
    "                combined_text = await self.async_crawler.crawl(domain_url)\n",
    "            except Exception as e:\n",
    "                return {\"error\": f\"Unhandled scraping failure: {str(e)}\", \"url\": input_str}\n",
    "\n",
    "            if not combined_text or len(combined_text.strip()) < 50:\n",
    "                return {\n",
    "                    \"error\": \"Unable to extract meaningful content from URL\",\n",
    "                    \"url\": input_str\n",
    "                }\n",
    "        else:\n",
    "            combined_text = input_str\n",
    "\n",
    "        # 2. Summarize (offload to executor)\n",
    "        summarized = await self.loop.run_in_executor(None, self.summarize_text, combined_text)\n",
    "        # If summarization was too short, use the raw combined_text instead\n",
    "        final_text = summarized if summarized != \"NaN\" else combined_text\n",
    "\n",
    "        # 3. Embed (offload to executor)\n",
    "        try:\n",
    "            embedding = await self.loop.run_in_executor(None, self.embed_text, final_text)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Embedding failure: {str(e)}\", \"url\": input_str}\n",
    "\n",
    "        # 4. Classify (offload to executor)\n",
    "        try:\n",
    "            result = await self.loop.run_in_executor(\n",
    "                None, self._classify_from_embedding, embedding, input_str, final_text\n",
    "            )\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Classification failure: {str(e)}\", \"url\": input_str}\n",
    "\n",
    "    def __call__(self, input_str: str) -> dict:\n",
    "        \"\"\"\n",
    "        Synchronous wrapper that runs the async pipeline to completion.\n",
    "        \"\"\"\n",
    "        return self.loop.run_until_complete(self.__call_async__(input_str))\n",
    "\n",
    "\n",
    "# Example: Classify 10 URLs from the Hugging Face dataset\n",
    "async def classify_multiple_websites(url_list):\n",
    "    predictor = SoACerPredictor(\n",
    "        embedder_name=\"meta-llama/Meta-Llama-3-8B\",\n",
    "        classifier_source=\"Shahriar/SoACer\",\n",
    "        classifier_local_dir=None,\n",
    "        device=None,\n",
    "        crawler_max_links=3,\n",
    "        crawler_concurrency=10,\n",
    "    )\n",
    "    tasks = [asyncio.create_task(predictor.__call_async__(url)) for url in url_list]\n",
    "    results = await asyncio.gather(*tasks, return_exceptions=False)\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    from datasets import load_dataset\n",
    "    import json\n",
    "\n",
    "    import nest_asyncio\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # (Assume classify_multiple_websites is already defined as before)\n",
    "\n",
    "    # 1. Load the Hugging Face dataset\n",
    "    dataset = load_dataset(\"Shahriar/SoAC_Corpus\", split=\"train\")\n",
    "\n",
    "    # 2. Extract the first 10 Privacy_Policy_URL values\n",
    "    urls = [dataset[i][\"Privacy_Policy_URL\"] for i in range(10)]\n",
    "\n",
    "    # 3. Get the currently running event loop\n",
    "    loop = asyncio.get_event_loop()\n",
    "\n",
    "    # 4. Schedule the coroutine using run_until_complete\n",
    "    all_results = loop.run_until_complete(classify_multiple_websites(urls))\n",
    "\n",
    "    # 5. Pretty-print each result\n",
    "    for idx, (url, res) in enumerate(zip(urls, all_results), start=1):\n",
    "        print(f\"\\n=== Result for URL #{idx}: {url} ===\")\n",
    "        print(json.dumps(res, indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26902697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "industry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
